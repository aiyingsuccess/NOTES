1. torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.
For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.

If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.


2. Structure

torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.
nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.
nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.
autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node
that connects to functions that created a Tensor and encodes its history.


3.funtion  .view(-1,3) -1 is the dimension size depends on 3 and the total numbers


4.function .data to get the raw number from tensor


5.Remember to first initialize the models and optimizers, then load the dictionary locally.

modelA = Net()
optimModelA = optim.SGD(modelA.parameters(), lr=0.001, momentum=0.9)


6.Saving and loading a general checkpoint model for inference or resuming training can be helpful for picking up where you last left off.
When saving a general checkpoint, you must save more than just the model’s state_dict. It is important to also save the optimizer’s state_dict,
as this contains buffers and parameters that are updated as the model trains. Other items that you may want to save are the epoch you left off on, 
the latest recorded training loss.


7.When loading a model on a GPU that was trained and saved on GPU, simply convert the initialized model to a CUDA optimized model using 
model.to(torch.device('cuda'))
 
 Note that calling my_tensor.to(device) returns a new copy of my_tensor on GPU. It does NOT overwrite my_tensor. Therefore, remember to manually 
overwrite tensors: my_tensor = my_tensor.to(torch.device('cuda')).


8.When loading a model on a GPU that was trained and saved on CPU, set the map_location argument in the torch.load() function to cuda:device_id. This loads
the model to a given GPU device.

Be sure to call model.to(torch.device('cuda')) to convert the model’s parameter tensors to CUDA tensors.

Finally, also be sure to use the .to(torch.device('cuda')) function on all model inputs to prepare the data for the CUDA optimized model.  
Make sure to call input = input.to(device) on any input tensors that you feed to the model and overwrite the input.


8.You can also use model.zero_grad(). This is the same as using optimizer.zero_grad() as long as all your model parameters are in that optimizer.
Use your best judgement to decide which one to use.


9. Sample code of backward:

net.zero_grad()     # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)


10
